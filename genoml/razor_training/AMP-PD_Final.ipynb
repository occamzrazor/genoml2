{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Logistic Regression on AMP-PD \n",
    "\n",
    "#### Authors: Maria Castanos and William Koehler\n",
    "\n",
    "## Introduction\n",
    "The objective of this analysis is to train a regularized logistic regression on the AMP-PD Dataset. However, to achieve computational efficiency, feature selecion is performed to reduce the dataset's dimensions, while maintaining the  necessary information to achieve the highest prediciton power. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "This analysis uses statistical tools to use gene variants to classify patients with Parkinson's Disease.\n",
    "\n",
    "### Dataset\n",
    "The dataset has a total of 2790 patients, out of which 1711 are diagnosed with Parkinson's. There is a total of 1,981,373 gene variants per patient. \n",
    "\n",
    "Gene variants can have the value of 0, 1, or 2, which mean the patient is homozygous for first allele, homozygous for second allele, and heterozygous, respectively.\n",
    "\n",
    "### Feature Selection\n",
    "[Various feature selection methods](https://scikit-learn.org/stable/modules/feature_selection.html) are performed to score the realtionships between the predictors and the dependent variable. Dimensionality reduction is then performed by picking the $k$ predictors with the highest $k$ scores. Specifically, relationships are scored using the methods described below.\n",
    "\n",
    "#### Tree Based Selection \n",
    "This method computes impurity-based feature importances to drop irrelevant features. \n",
    "\n",
    "#### Univariate Feature Selection\n",
    "This method chooses the best features based on univariate statistical tests that score the degree of dependency between the predictors and the dependent variable. The tests used where:\n",
    " - Chi-squared\n",
    " - ANOVA F-value\n",
    " - Mutual Information Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Tuning \n",
    "For each feature selection method, the training was done on the top $100$, $1000$, and $10000$ scored features.\n",
    "\n",
    "### Regularized Logistic Regression \n",
    "A regularized multinomial logistic regression is trained to predict two classes (Control, Case, Other). Elastic Net penalization was used in order to find a compromise between ridge and lasso penalizations. \n",
    "\n",
    "The optimization problem is:\n",
    "$$\\max_{\\beta_{0k}, \\beta_{k}} \\left\\{ \\sum_{i = 1}^{N} \\log Pr(g_i|x_i) - \\lambda \\sum_{k = 1}^K\\sum_{j = 1}^p \\big(\\alpha |\\beta_{kj}| + (1 - \\alpha)\\beta_{kj}^2\\big) \\right\\}$$ \n",
    "\n",
    "#### Hyperparameter Optimization\n",
    "Hyperparameters $\\lambda$ and $\\alpha$ in the problem above, are optimized by implementing a Stratified ShuffleSplit cross-validator. The optimal hyperparametrs are those that maximize the balanced accuracy:\n",
    "$$\\frac{1}{2}\\left( \\frac{TP}{TP + FN} + \\frac{TN}{FP + TN} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "The experiments described above were performed as described bellow.\n",
    "### Save and Load Data\n",
    "In order to construct the $X$ matrix containing the gene variants the [.pgen](https://s3.console.aws.amazon.com/s3/object/amp-pd-data?region=us-west-2&prefix=genomic-data/reduced/ldpruned_data.pgen) ( PLINK 2's preferred way to represent genotype calls) and [.psam](https://s3.console.aws.amazon.com/s3/object/amp-pd-data?region=us-west-2&prefix=genomic-data/reduced/ldpruned_data.psam) (sample information). The $y$ vector was constructed using the [labels file](https://s3.console.aws.amazon.com/s3/object/amp-pd-data?region=us-west-2&prefix=genomic-data/reduced/latest_labels.tsv). \n",
    "\n",
    "The [train_test_split.py](https://github.com/occamzrazor/genoml2/blob/feature/train_models/genoml/razor_training/train_test_split.py) script was ran to generate and save to disk the training and test splits for both the predictive variables ($X$ matrix) and the dependent variable ($y$ vector)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%run train_test_split.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from genoml import logistic_regression_experiments\n",
    "k = 100 #1000, 10000\n",
    "data_path = \"s3://amp-pd-data/genomic-data/logistic_regression_experiments/selected_{}\".format(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree Based Selection\n",
    "First we do feature selection with a tree based method to select the most important k features, according to the  impurity-based feature importances: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tks = TopKSelectorsExperiment.load_from_data(data_path)\n",
    "tks_tree = TreeSelectorExperiments.load_from_data(tks)\n",
    "top_k = tks_tree.fit_feature_selection_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Chi-Squared\n",
    "Similarly to the tree based method, we do feature selection with Chi-squared univariate tests to select the most important k features, according to these test scores: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tks_chi2 = TopKSelectorsExperiment.load_from_data(data_path)\n",
    "top_k = tks_chi2.fit_feature_selection_model(scoring_funct=chi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate ANOVA F-value"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tks_f = TopKSelectorsExperiment.load_from_data(data_path)\n",
    "top_k = tks_f.fit_feature_selection_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Logistic Regression \n",
    "For each method, we train the logistic regression on the selected features and save the resulting models to disk by running:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tks_tree.fit_logreg_experiments()\n",
    "tks_tree.save(experiment_dir)\n",
    "tks_chi2.fit_logreg_experiments()\n",
    "tks_chi2.save(experiment_dir)\n",
    "tks_f.fit_logreg_experiments()\n",
    "tks_f.save(experiment_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Various performance metrics for classification tasks are used to evaluate the results of the experiments described above. In particular:\n",
    "\n",
    "- *Accuracy:* Ratio between the number of correctly classified patients to the total number of patients.\n",
    "- *Precision:* Ratio between the correctly classified Parkinson's patients to the total classified Parkinson's patients.\n",
    "- *Recall:* Ratio between the correctly classified Parkinson's patients to the total patients with Parkinson's disease on the dataset.\n",
    "- *F1-Score:* Harmonic mean of precision and recall. It balances both the concerns of precision and recall in one number.\n",
    "- *Balanced Accuracy:* It's the average of the within class accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from results import results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "performance_table = results.performance_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tree 100</th>\n",
       "      <th>Tree 1000</th>\n",
       "      <th>Tree 10000</th>\n",
       "      <th>Chi2 100</th>\n",
       "      <th>Chi2 1000</th>\n",
       "      <th>Chi2 10000</th>\n",
       "      <th>Ftest 100</th>\n",
       "      <th>Ftest 1000</th>\n",
       "      <th>Ftest 10000</th>\n",
       "      <th>Random</th>\n",
       "      <th>Majority Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.750</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.791</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.768</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.776</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-Score</th>\n",
       "      <td>0.779</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <td>0.746</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Tree 100  Tree 1000  Tree 10000  Chi2 100  Chi2 1000  \\\n",
       "Accuracy              0.750      0.726       0.688     0.745      0.740   \n",
       "Precision             0.791      0.778       0.729     0.785      0.789   \n",
       "Recall                0.768      0.733       0.729     0.768      0.749   \n",
       "F1-Score              0.779      0.755       0.729     0.776      0.768   \n",
       "Balanced Accuracy     0.746      0.725       0.681     0.741      0.738   \n",
       "\n",
       "                   Chi2 10000  Ftest 100  Ftest 1000  Ftest 10000  Random  \\\n",
       "Accuracy                0.695      0.749       0.728        0.694   0.576   \n",
       "Precision               0.725      0.787       0.779        0.716   0.576   \n",
       "Recall                  0.756      0.772       0.737        0.776   1.000   \n",
       "F1-Score                0.740      0.780       0.758        0.745   0.731   \n",
       "Balanced Accuracy       0.684      0.744       0.727        0.679   0.500   \n",
       "\n",
       "                   Majority Class  \n",
       "Accuracy                    0.481  \n",
       "Precision                   0.556  \n",
       "Recall                      0.497  \n",
       "F1-Score                    0.525  \n",
       "Balanced Accuracy           0.479  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_table.index = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Balanced Accuracy'] \n",
    "performance_table.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our dataset is somewhat balanced, accuracy is a reasonable metric to evaluate. However, since the number positives and the number of negatives is still different, Balanced Accuracy should be observed more in detail.\n",
    "We observe that as more predictors are added into the models, both the *Accuracy* and the *Balanced Accuracy* metrics deteriorate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $k = 100$, the tree-based method yields the highest *Accuracy* and *Balanced Accuracy* the other methods on every metric; however, results are don't differ much. The tree-based method (with $k = 100$), also outpreforms other methods on precision, recall, and f1-score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Important Features\n",
    "We are also interested in outputting the coeffitients of the selected features in order to see the impact on the log odds ratio. In particular, we show the column numbers ordered from highest to lower coefficient. \n",
    "#### Tree-Based Method (k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17, 79, 83, 35,  1, 10, 28, 41, 11,  3,  0, 98,  2, 92, 76,  6,\n",
       "        70, 93, 64, 31, 61, 12,  5, 40, 36, 95, 39, 63, 94, 19, 23,  8,\n",
       "        42, 71, 24, 86, 13, 67, 59,  7, 29, 38,  9, 34, 49, 25, 47, 32,\n",
       "        51, 20, 30, 62, 43, 27, 14, 26, 90, 85, 15, 21, 45, 22, 69, 81,\n",
       "        82, 46, 72,  4, 56, 65, 44, 68, 48, 74, 33, 99, 57, 54, 50, 84,\n",
       "        88, 75, 16, 66, 91, 60, 58, 97, 87, 78, 37, 89, 18, 96, 52, 77,\n",
       "        80, 53, 55, 73]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff = results.get_coefficients('tree', 100)\n",
    "np.argsort(-np.abs(coeff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Chi-Squared (k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55, 89, 77,  0, 32, 65, 97, 34, 78, 69, 44,  4, 58,  6, 37, 73,\n",
       "        80, 57, 49, 94, 62, 28,  2, 66, 51, 23, 26,  8, 88, 60, 90,  1,\n",
       "        15, 48, 13, 46, 82, 29, 85, 50, 81, 59, 11, 25, 40, 45, 35, 27,\n",
       "        96, 24, 18,  5, 20, 14, 54, 91, 31, 71, 67, 10, 86, 76, 30, 68,\n",
       "        36, 52, 38, 16, 99, 56, 93, 70, 43, 12, 41, 61,  9, 64, 74,  3,\n",
       "        19, 17, 42, 47, 72, 39,  7, 92, 98, 22, 63, 33, 53, 95, 83, 84,\n",
       "        87, 21, 79, 75]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff = results.get_coefficients('univariate_chi2', 100)\n",
    "np.argsort(-np.abs(coeff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate ANOVA F-Value (k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  0,  4,  5,  3,  1, 18, 19,  6, 11, 10, 26,  9,  8, 13, 45,\n",
       "        32, 20, 22, 23, 48, 24, 41,  7, 38, 62, 16, 21, 37, 49, 27, 28,\n",
       "        40, 70, 17, 15, 36, 66, 98, 47, 99, 43, 30, 14, 39, 67, 71, 68,\n",
       "        58, 12, 25, 53, 78, 75, 79, 81, 73, 94, 44, 91, 29, 77, 93, 60,\n",
       "        33, 31, 55, 46, 92, 63, 83, 84, 51, 89, 50, 74, 85, 72, 34, 35,\n",
       "        56, 80, 95, 65, 88, 57, 52, 86, 76, 87, 54, 59, 82, 96, 90, 61,\n",
       "        69, 42, 64, 97]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff = results.get_coefficients('univariate_fclassif', 100)\n",
    "np.argsort(-np.abs(coeff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Since our goal is to achieve the most compact and poweful combination of features, we explored the implementation of PCA on the reduced datasets and then perform logistic regression on the resulting datasets. \n",
    "\n",
    "The results of such experiments aree displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tree 100</th>\n",
       "      <th>Tree 1000</th>\n",
       "      <th>Tree 10000</th>\n",
       "      <th>Chi2 100</th>\n",
       "      <th>Chi2 1000</th>\n",
       "      <th>Chi2 10000</th>\n",
       "      <th>Ftest 100</th>\n",
       "      <th>Ftest 1000</th>\n",
       "      <th>Ftest 10000</th>\n",
       "      <th>Random</th>\n",
       "      <th>Majority Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.727</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.777</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.788</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.739</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.756</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1-Score</th>\n",
       "      <td>0.757</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <td>0.725</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.738</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Tree 100  Tree 1000  Tree 10000  Chi2 100  Chi2 1000  \\\n",
       "Accuracy              0.727      0.722       0.687     0.740      0.740   \n",
       "Precision             0.777      0.774       0.731     0.780      0.787   \n",
       "Recall                0.739      0.729       0.721     0.762      0.750   \n",
       "F1-Score              0.757      0.751       0.726     0.771      0.768   \n",
       "Balanced Accuracy     0.725      0.720       0.681     0.736      0.738   \n",
       "\n",
       "                   Chi2 10000  Ftest 100  Ftest 1000  Ftest 10000  Random  \\\n",
       "Accuracy                0.654      0.749       0.726        0.687   0.576   \n",
       "Precision               0.705      0.788       0.782        0.716   0.576   \n",
       "Recall                  0.686      0.770       0.727        0.756   1.000   \n",
       "F1-Score                0.696      0.779       0.754        0.736   0.731   \n",
       "Balanced Accuracy       0.649      0.745       0.726        0.674   0.500   \n",
       "\n",
       "                   Majority Class  \n",
       "Accuracy                    0.513  \n",
       "Precision                   0.589  \n",
       "Recall                      0.509  \n",
       "F1-Score                    0.546  \n",
       "Balanced Accuracy           0.514  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_table = results.performance_table(do_pca = 'do_pca')\n",
    "performance_table.index = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Balanced Accuracy'] \n",
    "performance_table.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When projecting the data onto a lower space, results change very little. This can mean that $95\\%$ of the variance contains most of the power to predict $y$. For each feature selection method and each k, we explain how PCA reduces dimensions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $k = 100$, the Univariate method with ANOVA F-values outperforms the others on every metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree\n",
    "When performing PCA on the top k features,  $k = 100$, picked by the tree-based method, the dataset is reduced to $80$ features by PCA.\n",
    "\n",
    "For $k = 1000$, the dataset is reduced to $674$ features by PCA.\n",
    "\n",
    "For $k = 10000$, the dataset is reduced to $2369$ features by PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Chi-Squared\n",
    "When performing PCA on the top k features,  $k = 100$, picked by the chi-squared tests, the dataset is reduced to $83$ features by PCA.\n",
    "\n",
    "For $k = 1000$, the dataset is reduced to $687$ features by PCA.\n",
    "\n",
    "For $k = 10000$, the dataset is reduced to $2302$ features by PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate ANOVA F-Value\n",
    "When performing PCA on the top k features,  $k = 100$, picked by the F-tests, the dataset is reduced to $82$ features by PCA.\n",
    "\n",
    "For $k = 1000$, the dataset is reduced to $716$ features by PCA.\n",
    "\n",
    "For $k = 10000$, the dataset is reduced to $2370$ features by PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be verified with the [files generated](https://s3.console.aws.amazon.com/s3/buckets/amp-pd-data?region=us-west-2&prefix=genomic-data/reduced/results/PCA/&showversions=false) from running the [experiments_pca.py](https://github.com/occamzrazor/genoml2/blob/feature/train_models/genoml/razor_training/experiments_pca.py) script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "The results presented above can also be replicated by running, the [experiments.py](https://github.com/occamzrazor/genoml2/blob/feature/train_models/genoml/razor_training/experiments.py) and [results.py](https://github.com/occamzrazor/genoml2/blob/feature/train_models/genoml/razor_training/results.py) scripts. To generate the table with the PCA results, run [experiments_pca.py](https://github.com/occamzrazor/genoml2/blob/feature/train_models/genoml/razor_training/experiments_pca.py) (after running [experiments.py](https://github.com/occamzrazor/genoml2/blob/feature/train_models/genoml/razor_training/experiments.py)) and then run [results.py](https://github.com/occamzrazor/genoml2/blob/feature/train_models/genoml/razor_training/results.py) script with do_pca='do_pca'.\n",
    "\n",
    "However, these scripts were refactored into the [logistic_regression_eexperiments.py](https://github.com/occamzrazor/genoml2/blob/feature/logreg_experiments/genoml/razor_training/logistic_regression_experiments.py) script, and its usage is described in the Experiments Section of this Notebook.\n",
    "\n",
    "Mutual Information Score was not implementeed due to a lack of computational power. In the future, if we want to add this analysis, a bigger machine should be used to run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
